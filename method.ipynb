{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "This section outlines our approach to evaluating the impact of different imputation methods on predictive modeling for heart failure patients in MIMIC-IV.\n",
    "\n",
    "## Overview\n",
    "1. Feature Selection: LASSO → XGBoost\n",
    "2. Missing Data Handling: GPLVM Imputation\n",
    "3. Model Development: Baseline (LR, RF) vs Primary (XGBoost)\n",
    "4. Model Interpretation: SHAP Analysis\n",
    "5. Hyperparameter Tuning: GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Selection: LASSO → XGBoost\n",
    "\n",
    "### Feature Selection Pipeline\n",
    "- Initial feature selection using LASSO regression to identify the most important predictors\n",
    "- Further refinement using XGBoost feature importance\n",
    "- Final feature set used across all imputation methods for fair comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.contrib.gp as gp\n",
    "import pyro.distributions as dist\n",
    "import pyro.ops.stats as stats\n",
    "from torch.nn import Parameter\n",
    "from sklearn.linear_model import LassoCV, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def feature_selection_pipeline(data, target_col):\n",
    "    \"\"\"\n",
    "    Two-step feature selection using LASSO and XGBoost\n",
    "    \"\"\"\n",
    "    # Split data\n",
    "    X = data.drop(columns=[target_col])\n",
    "    y = data[target_col]\n",
    "    \n",
    "    # LASSO feature selection\n",
    "    lasso = LassoCV(cv=5)\n",
    "    lasso.fit(X, y)\n",
    "    \n",
    "    # Get non-zero coefficients\n",
    "    lasso_features = X.columns[lasso.coef_ != 0]\n",
    "    \n",
    "    # XGBoost feature importance\n",
    "    xgb_model = xgb.XGBClassifier()\n",
    "    xgb_model.fit(X[lasso_features], y)\n",
    "    \n",
    "    # Get feature importance\n",
    "    importance = pd.DataFrame({\n",
    "        'feature': lasso_features,\n",
    "        'importance': xgb_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    return importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Missing Data Handling\n",
    "\n",
    "\n",
    "### Imputation Methods\n",
    "We evaluate three different imputation approaches:\n",
    "1. **Mean/Mode Imputation**: Simple baseline method\n",
    "2. **Regression-based Imputation**: Using predictive models for each feature\n",
    "\n",
    "### Missingness Scenarios\n",
    "- Full data (0% missing)\n",
    "- Subset with 0% missing values\n",
    "- Subset with 20% missing values\n",
    "- Subset with 40% missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_missing_data(data, missing_percentage):\n",
    "    # Create missing values in the dataset\n",
    "    mask = np.random.random(data.shape) < missing_percentage\n",
    "    data_missing = data.copy()\n",
    "    data_missing[mask] = np.nan\n",
    "    return data_missing\n",
    "\n",
    "def mean_mode_imputation(data):\n",
    "    # Separate numeric and categorical columns\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # Create imputers\n",
    "    numeric_imputer = SimpleImputer(strategy='mean')\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    \n",
    "    # Impute data\n",
    "    data_imputed = data.copy()\n",
    "    if len(numeric_cols) > 0:\n",
    "        data_imputed[numeric_cols] = numeric_imputer.fit_transform(data[numeric_cols])\n",
    "    if len(categorical_cols) > 0:\n",
    "        data_imputed[categorical_cols] = categorical_imputer.fit_transform(data[categorical_cols])\n",
    "    \n",
    "    return data_imputed\n",
    "\n",
    "def regression_imputation(data):\n",
    "    # Use Random Forest for imputation\n",
    "    imputer = IterativeImputer(\n",
    "        estimator=RandomForestRegressor(),\n",
    "        max_iter=10,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Impute only numeric columns\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    data_imputed = data.copy()\n",
    "    data_imputed[numeric_cols] = imputer.fit_transform(data[numeric_cols])\n",
    "    \n",
    "    return data_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPLVM Imputation**: Advanced deep learning-based approach\n",
    "\n",
    "The Gaussian Process Latent Variable Model (GPLVM) implementation includes:\n",
    "\n",
    "1. **Model Structure**:\n",
    "   - Sparse GP regression for efficient computation\n",
    "   - RBF kernel for smooth latent space representation\n",
    "   - Two-dimensional latent space for visualization\n",
    "\n",
    "2. **Key Components**:\n",
    "   - Prior mean initialization for latent variables\n",
    "   - Inducing points for sparse approximation\n",
    "   - Automatic guide for variational inference\n",
    "\n",
    "3. **Features**:\n",
    "   - Imputation of missing values with uncertainty estimates\n",
    "   - Visualization of training progress and latent space\n",
    "   - Support for both continuous and categorical variables\n",
    "\n",
    "4. **Usage Example**:\n",
    "```python\n",
    "# Example usage\n",
    "results = gplvm_svm_pipeline(\n",
    "    data=your_data,\n",
    "    target_col='target',\n",
    "    missing_percentage=0.2,\n",
    "    latent_dim=2,\n",
    "    num_inducing=32,\n",
    "    num_steps=4000\n",
    ")\n",
    "\n",
    "# Visualize results\n",
    "visualize_gplvm_svm_results(\n",
    "    results,\n",
    "    X=your_data.drop(columns=['target']),\n",
    "    feature_names=your_feature_names\n",
    ")\n",
    "\n",
    "The Constrained SVM approach is a method that takes into account the uncertainty in the imputed values when making predictions. \n",
    "\n",
    "Instead of using a single imputed dataset, we generate multiple possible datasets based on the uncertainty estimates from GPLVM\n",
    "\n",
    "Each dataset represents a different possible realization of the true values\n",
    "We train an SVM on each dataset and then select the optimal model\n",
    "\n",
    "**Key features**:\n",
    "\n",
    "- We use package from https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/gplvm.ipynb\n",
    "- GPLVM handles missing value imputation with uncertainty estimates\n",
    "- Constrained SVM uses these uncertainty estimates to create robust models\n",
    "- SHAP analysis helps interpret the model's decisions\n",
    "- Visualization tools help understand the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gplvm_imputation(data, latent_dim=2, num_inducing=32, num_steps=4000):\n",
    "    \"\"\"\n",
    "    GPLVM-based imputation for missing values\n",
    "    \"\"\"\n",
    "    # Convert data to torch tensor\n",
    "    data_tensor = torch.tensor(data.values, dtype=torch.get_default_dtype())\n",
    "    y = data_tensor.t()\n",
    "    \n",
    "    # Create prior mean for latent variables\n",
    "    X_prior_mean = torch.zeros(y.size(1), latent_dim)\n",
    "    \n",
    "    # Define RBF kernel\n",
    "    kernel = gp.kernels.RBF(input_dim=latent_dim, lengthscale=torch.ones(latent_dim))\n",
    "    \n",
    "    # Initialize latent variables\n",
    "    X = Parameter(X_prior_mean.clone())\n",
    "    \n",
    "    # Initialize inducing points\n",
    "    Xu = stats.resample(X_prior_mean.clone(), num_inducing)\n",
    "    \n",
    "    # Create sparse GP model\n",
    "    gplvm = gp.models.SparseGPRegression(\n",
    "        X, y, kernel, Xu,\n",
    "        noise=torch.tensor(0.01),\n",
    "        jitter=1e-5\n",
    "    )\n",
    "    \n",
    "    # Set up prior for X\n",
    "    gplvm.X = pyro.nn.PyroSample(\n",
    "        dist.Normal(X_prior_mean, 0.1).to_event()\n",
    "    )\n",
    "    \n",
    "    # Set up guide\n",
    "    gplvm.autoguide(\"X\", dist.Normal)\n",
    "    \n",
    "    # Train the model\n",
    "    losses = gp.util.train(gplvm, num_steps=num_steps)\n",
    "    \n",
    "    # Get imputed values and uncertainty\n",
    "    gplvm.mode = \"guide\"\n",
    "    X = gplvm.X_loc.detach().numpy()\n",
    "    \n",
    "    # Reconstruct data with uncertainty\n",
    "    imputed_data = data.copy()\n",
    "    uncertainty = pd.DataFrame(index=data.index, columns=data.columns, dtype=float)\n",
    "    \n",
    "    # Use the learned latent space to impute missing values\n",
    "    for i in range(data.shape[1]):\n",
    "        if data.iloc[:, i].isnull().any():\n",
    "            # Get predictions for missing values\n",
    "            pred_mean, pred_var = gplvm.forward(X)\n",
    "            imputed_data.iloc[:, i] = pred_mean.detach().numpy()\n",
    "            uncertainty.iloc[:, i] = pred_var.detach().numpy()\n",
    "    \n",
    "    return imputed_data, uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3, 4: Model Development and Model Interpretation: SHAP Analysis: Baseline (LR, RF) vs Primary (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train baseline and primary models with hyperparameter tuning\n",
    "    \"\"\"\n",
    "    # Define models\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "        'Random Forest': RandomForestClassifier(),\n",
    "        'XGBoost': xgb.XGBClassifier()\n",
    "    }\n",
    "    \n",
    "    # Define parameter grids for GridSearchCV\n",
    "    param_grids = {\n",
    "        'Logistic Regression': {'C': [0.1, 1, 10]},\n",
    "        'Random Forest': {'n_estimators': [100, 200], 'max_depth': [None, 10]},\n",
    "        'XGBoost': {'n_estimators': [100, 200], 'max_depth': [3, 6]}\n",
    "    }\n",
    "    \n",
    "    # Store SHAP values\n",
    "    shap_values = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Perform grid search\n",
    "        grid_search = GridSearchCV(model, param_grids[name], cv=5, scoring='roc_auc')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Get best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "        models[name] = best_model\n",
    "        \n",
    "        # Calculate SHAP values\n",
    "        if name == 'XGBoost':\n",
    "            explainer = shap.TreeExplainer(best_model)\n",
    "            shap_values[name] = explainer.shap_values(X_test)\n",
    "        elif name == 'Random Forest':\n",
    "            explainer = shap.TreeExplainer(best_model)\n",
    "            shap_values[name] = explainer.shap_values(X_test)\n",
    "        else:  # Logistic Regression\n",
    "            explainer = shap.LinearExplainer(best_model, X_train)\n",
    "            shap_values[name] = explainer.shap_values(X_test)\n",
    "    \n",
    "    return models, shap_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Analysis Pipeline\n",
    "\n",
    "This implementation follows all requirements for predicting in-hospital mortality for ICU HF patients:\n",
    "\n",
    "1. **Feature Selection**\n",
    "   - Two-step reduction using LASSO and XGBoost\n",
    "   - Identifies most important predictors\n",
    "\n",
    "2. **Missing Data Handling**\n",
    "   - GPLVM imputation with uncertainty estimation\n",
    "   - Handles different missingness levels (0%, 20%, 40%)\n",
    "\n",
    "3. **Model Development**\n",
    "   - Baseline models (Logistic Regression, Random Forest)\n",
    "   - Primary model (XGBoost)\n",
    "   - Constrained SVM with uncertainty incorporation\n",
    "\n",
    "4. **Model Interpretation**\n",
    "   - SHAP analysis for feature importance\n",
    "   - Comparison across missingness levels\n",
    "   - Uncertainty visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrained_svm(X, y, uncertainty, n_samples=10):\n",
    "    \"\"\"\n",
    "    Constrained SVM implementation that incorporates uncertainty\n",
    "    \"\"\"\n",
    "    # Store all trained models and their performance\n",
    "    models = []\n",
    "    scores = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # Generate a new dataset by sampling from uncertainty distribution\n",
    "        X_sampled = X + np.random.normal(0, np.sqrt(uncertainty))\n",
    "        \n",
    "        # Train SVM on this sampled dataset\n",
    "        model = SVC(kernel='rbf', probability=True)\n",
    "        model.fit(X_sampled, y)\n",
    "        \n",
    "        # Evaluate model performance\n",
    "        score = cross_val_score(model, X_sampled, y, cv=5).mean()\n",
    "        \n",
    "        models.append(model)\n",
    "        scores.append(score)\n",
    "    \n",
    "    # Select the best performing model\n",
    "    best_idx = np.argmax(scores)\n",
    "    best_model = models[best_idx]\n",
    "    \n",
    "    return best_model, scores[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_analysis_pipeline(data, target_col):\n",
    "    \"\"\"\n",
    "    Complete analysis pipeline following all requirements\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Feature Selection\n",
    "    print(\"Performing feature selection...\")\n",
    "    importance = feature_selection_pipeline(data, target_col)\n",
    "    \n",
    "    # 2. Analyze different missingness levels\n",
    "    for missing_pct in [0, 0.2, 0.4]:\n",
    "        print(f\"\\nProcessing {missing_pct*100}% missing data...\")\n",
    "        \n",
    "        # Create missing data\n",
    "        data_missing = create_missing_data(data, missing_pct)\n",
    "        \n",
    "        # GPLVM imputation\n",
    "        imputed_data, uncertainty = gplvm_imputation(data_missing)\n",
    "        \n",
    "        # Prepare data for modeling\n",
    "        X = imputed_data.drop(columns=[target_col])\n",
    "        y = imputed_data[target_col]\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Train models\n",
    "        models, shap_values = train_models(X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        # Train constrained SVM\n",
    "        best_svm, svm_score = constrained_svm(X_train, y_train, uncertainty)\n",
    "        \n",
    "        # Store results\n",
    "        results[missing_pct] = {\n",
    "            'feature_importance': importance,\n",
    "            'imputed_data': imputed_data,\n",
    "            'uncertainty': uncertainty,\n",
    "            'models': models,\n",
    "            'shap_values': shap_values,\n",
    "            'svm_model': best_svm,\n",
    "            'svm_score': svm_score\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def visualize_results(results, X, feature_names):\n",
    "    \"\"\"\n",
    "    Visualize results from analysis\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: Feature importance across missingness levels\n",
    "    plt.subplot(131)\n",
    "    for missing_pct, result in results.items():\n",
    "        importance = result['feature_importance']\n",
    "        plt.plot(importance['importance'], \n",
    "                label=f'{missing_pct*100}% missing')\n",
    "    plt.title('Feature Importance Across Missingness Levels')\n",
    "    plt.xlabel('Feature Rank')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot 2: SHAP summary plot\n",
    "    plt.subplot(132)\n",
    "    if isinstance(results[0]['shap_values']['XGBoost'], list):\n",
    "        shap_values = results[0]['shap_values']['XGBoost'][0]\n",
    "    else:\n",
    "        shap_values = results[0]['shap_values']['XGBoost']\n",
    "    shap.summary_plot(shap_values, X, feature_names=feature_names, show=False)\n",
    "    plt.title(\"SHAP Summary Plot\")\n",
    "    \n",
    "    # Plot 3: Uncertainty distribution\n",
    "    plt.subplot(133)\n",
    "    plt.hist(results[0]['uncertainty'].values.flatten(), bins=50)\n",
    "    plt.title(\"Distribution of Imputation Uncertainty\")\n",
    "    plt.xlabel(\"Uncertainty\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cpsc419)",
   "language": "python",
   "name": "cpsc419"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
