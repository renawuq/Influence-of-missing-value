{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "This section outlines our approach to evaluating the impact of different imputation methods on predictive modeling for heart failure patients in MIMIC-IV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing and Feature Selection\n",
    "\n",
    "### Feature Selection Pipeline\n",
    "- Initial feature selection using LASSO regression to identify the most important predictors\n",
    "- Further refinement using XGBoost feature importance\n",
    "- Final feature set used across all imputation methods for fair comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LassoCV\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def feature_selection_pipeline(data, target_col):\n",
    "    # Split data\n",
    "    X = data.drop(columns=[target_col])\n",
    "    y = data[target_col]\n",
    "    \n",
    "    # LASSO feature selection\n",
    "    lasso = LassoCV(cv=5)\n",
    "    lasso.fit(X, y)\n",
    "    \n",
    "    # Get non-zero coefficients\n",
    "    lasso_features = X.columns[lasso.coef_ != 0]\n",
    "    \n",
    "    # XGBoost feature importance\n",
    "    xgb_model = xgb.XGBClassifier()\n",
    "    xgb_model.fit(X[lasso_features], y)\n",
    "    \n",
    "    # Get feature importance\n",
    "    importance = pd.DataFrame({\n",
    "        'feature': lasso_features,\n",
    "        'importance': xgb_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    return importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Missing Data Handling\n",
    "\n",
    "### Imputation Methods\n",
    "We evaluate three different imputation approaches:\n",
    "1. **Mean/Mode Imputation**: Simple baseline method\n",
    "2. **Regression-based Imputation**: Using predictive models for each feature\n",
    "3. **GPLVM Imputation**: Advanced deep learning-based approach\n",
    "\n",
    "### Missingness Scenarios\n",
    "- Full data (0% missing)\n",
    "- Subset with 0% missing values\n",
    "- Subset with 20% missing values\n",
    "- Subset with 40% missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def create_missing_data(data, missing_percentage):\n",
    "    # Create missing values in the dataset\n",
    "    mask = np.random.random(data.shape) < missing_percentage\n",
    "    data_missing = data.copy()\n",
    "    data_missing[mask] = np.nan\n",
    "    return data_missing\n",
    "\n",
    "def mean_mode_imputation(data):\n",
    "    # Separate numeric and categorical columns\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # Create imputers\n",
    "    numeric_imputer = SimpleImputer(strategy='mean')\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    \n",
    "    # Impute data\n",
    "    data_imputed = data.copy()\n",
    "    if len(numeric_cols) > 0:\n",
    "        data_imputed[numeric_cols] = numeric_imputer.fit_transform(data[numeric_cols])\n",
    "    if len(categorical_cols) > 0:\n",
    "        data_imputed[categorical_cols] = categorical_imputer.fit_transform(data[categorical_cols])\n",
    "    \n",
    "    return data_imputed\n",
    "\n",
    "def regression_imputation(data):\n",
    "    # Use Random Forest for imputation\n",
    "    imputer = IterativeImputer(\n",
    "        estimator=RandomForestRegressor(),\n",
    "        max_iter=10,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Impute only numeric columns\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    data_imputed = data.copy()\n",
    "    data_imputed[numeric_cols] = imputer.fit_transform(data[numeric_cols])\n",
    "    \n",
    "    return data_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPLVM Implementation Details\n",
    "\n",
    "The Gaussian Process Latent Variable Model (GPLVM) implementation includes:\n",
    "\n",
    "1. **Model Structure**:\n",
    "   - Sparse GP regression for efficient computation\n",
    "   - RBF kernel for smooth latent space representation\n",
    "   - Two-dimensional latent space for visualization\n",
    "\n",
    "2. **Key Components**:\n",
    "   - Prior mean initialization for latent variables\n",
    "   - Inducing points for sparse approximation\n",
    "   - Automatic guide for variational inference\n",
    "\n",
    "3. **Features**:\n",
    "   - Imputation of missing values with uncertainty estimates\n",
    "   - Visualization of training progress and latent space\n",
    "   - Support for both continuous and categorical variables\n",
    "\n",
    "4. **Usage Example**:\n",
    "```python\n",
    "# Example usage\n",
    "imputed_data, uncertainty = gplvm_imputation(\n",
    "    data=your_data,\n",
    "    latent_dim=2,\n",
    "    num_inducing=32,\n",
    "    num_steps=4000\n",
    ")\n",
    "\n",
    "\n",
    "1. Provides a complete GPLVM-based imputation method\n",
    "2. Includes uncertainty estimation for imputed values\n",
    "3. We use package from https://github.com/pyro-ppl/pyro/blob/dev/tutorial/source/gplvm.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "import pyro.contrib.gp as gp\n",
    "import pyro.distributions as dist\n",
    "import pyro.ops.stats as stats\n",
    "from torch.nn import Parameter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def gplvm_imputation(data, latent_dim=2, num_inducing=32, num_steps=4000):\n",
    "    \"\"\"\n",
    "    GPLVM-based imputation for missing values\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas DataFrame\n",
    "        Input data with missing values\n",
    "    latent_dim : int\n",
    "        Dimension of the latent space\n",
    "    num_inducing : int\n",
    "        Number of inducing points for sparse GP\n",
    "    num_steps : int\n",
    "        Number of optimization steps\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    imputed_data : pandas DataFrame\n",
    "        Data with imputed values\n",
    "    uncertainty : pandas DataFrame\n",
    "        Uncertainty estimates for imputed values\n",
    "    \"\"\"\n",
    "    # Convert data to torch tensor\n",
    "    data_tensor = torch.tensor(data.values, dtype=torch.get_default_dtype())\n",
    "    y = data_tensor.t()\n",
    "    \n",
    "    # Create prior mean for latent variables\n",
    "    X_prior_mean = torch.zeros(y.size(1), latent_dim)\n",
    "    \n",
    "    # Define RBF kernel\n",
    "    kernel = gp.kernels.RBF(input_dim=latent_dim, lengthscale=torch.ones(latent_dim))\n",
    "    \n",
    "    # Initialize latent variables\n",
    "    X = Parameter(X_prior_mean.clone())\n",
    "    \n",
    "    # Initialize inducing points\n",
    "    Xu = stats.resample(X_prior_mean.clone(), num_inducing)\n",
    "    \n",
    "    # Create sparse GP model\n",
    "    gplvm = gp.models.SparseGPRegression(\n",
    "        X, y, kernel, Xu,\n",
    "        noise=torch.tensor(0.01),\n",
    "        jitter=1e-5\n",
    "    )\n",
    "    \n",
    "    # Set up prior for X\n",
    "    gplvm.X = pyro.nn.PyroSample(\n",
    "        dist.Normal(X_prior_mean, 0.1).to_event()\n",
    "    )\n",
    "    \n",
    "    # Set up guide\n",
    "    gplvm.autoguide(\"X\", dist.Normal)\n",
    "    \n",
    "    # Train the model\n",
    "    losses = gp.util.train(gplvm, num_steps=num_steps)\n",
    "    \n",
    "    # Get imputed values and uncertainty\n",
    "    gplvm.mode = \"guide\"\n",
    "    X = gplvm.X_loc.detach().numpy()\n",
    "    \n",
    "    # Reconstruct data with uncertainty\n",
    "    imputed_data = data.copy()\n",
    "    uncertainty = pd.DataFrame(index=data.index, columns=data.columns, dtype=float)\n",
    "    \n",
    "    # Use the learned latent space to impute missing values\n",
    "    for i in range(data.shape[1]):\n",
    "        if data.iloc[:, i].isnull().any():\n",
    "            # Get predictions for missing values\n",
    "            pred_mean, pred_var = gplvm.forward(X)\n",
    "            imputed_data.iloc[:, i] = pred_mean.detach().numpy()\n",
    "            uncertainty.iloc[:, i] = pred_var.detach().numpy()\n",
    "    \n",
    "    return imputed_data, uncertainty\n",
    "\n",
    "def plot_gplvm_results(losses, X, labels=None):\n",
    "    \"\"\"\n",
    "    Plot GPLVM training results and latent space visualization\n",
    "    \"\"\"\n",
    "    # Plot loss curve\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(losses)\n",
    "    plt.title(\"GPLVM Training Loss\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    \n",
    "    # Plot latent space\n",
    "    plt.subplot(1, 2, 2)\n",
    "    if labels is not None:\n",
    "        colors = plt.get_cmap(\"tab10\").colors\n",
    "        for i, label in enumerate(labels.unique()):\n",
    "            mask = labels == label\n",
    "            plt.scatter(X[mask, 0], X[mask, 1], c=[colors[i]], label=label)\n",
    "        plt.legend()\n",
    "    else:\n",
    "        plt.scatter(X[:, 0], X[:, 1])\n",
    "    \n",
    "    plt.title(\"GPLVM Latent Space\")\n",
    "    plt.xlabel(\"Dimension 1\")\n",
    "    plt.ylabel(\"Dimension 2\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Constrained SVM approach is a method that takes into account the uncertainty in the imputed values when making predictions. \n",
    "\n",
    "Instead of using a single imputed dataset, we generate multiple possible datasets based on the uncertainty estimates from GPLVM\n",
    "\n",
    "Each dataset represents a different possible realization of the true values\n",
    "We train an SVM on each dataset and then select the optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrained_svm(X, y, uncertainty, n_samples=10):\n",
    "    \"\"\"\n",
    "    Constrained SVM implementation that incorporates uncertainty\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy array\n",
    "        Feature matrix (including imputed values)\n",
    "    y : numpy array\n",
    "        Target labels\n",
    "    uncertainty : numpy array\n",
    "        Uncertainty estimates from GPLVM\n",
    "    n_samples : int\n",
    "        Number of possible datasets to generate\n",
    "    \"\"\"\n",
    "    # Store all trained models and their performance\n",
    "    models = []\n",
    "    scores = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # Generate a new dataset by sampling from uncertainty distribution\n",
    "        X_sampled = X + np.random.normal(0, np.sqrt(uncertainty))\n",
    "        \n",
    "        # Train SVM on this sampled dataset\n",
    "        model = SVC(kernel='rbf')\n",
    "        model.fit(X_sampled, y)\n",
    "        \n",
    "        # Evaluate model performance\n",
    "        score = cross_val_score(model, X_sampled, y, cv=5).mean()\n",
    "        \n",
    "        models.append(model)\n",
    "        scores.append(score)\n",
    "    \n",
    "    # Select the best performing model\n",
    "    best_idx = np.argmax(scores)\n",
    "    best_model = models[best_idx]\n",
    "    \n",
    "    return best_model, scores[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_constrained_svm(X, y, uncertainty, best_model):\n",
    "    \"\"\"\n",
    "    Visualize the constrained SVM process\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: Original data with uncertainty\n",
    "    plt.subplot(131)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.5)\n",
    "    plt.title(\"Original Data with Uncertainty\")\n",
    "    \n",
    "    # Plot 2: Sampled datasets\n",
    "    plt.subplot(132)\n",
    "    for _ in range(5):  # Show 5 sampled datasets\n",
    "        X_sampled = X + np.random.normal(0, np.sqrt(uncertainty))\n",
    "        plt.scatter(X_sampled[:, 0], X_sampled[:, 1], c=y, alpha=0.2)\n",
    "    plt.title(\"Multiple Sampled Datasets\")\n",
    "    \n",
    "    # Plot 3: Decision boundary of best model\n",
    "    plt.subplot(133)\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                         np.arange(y_min, y_max, 0.1))\n",
    "    Z = best_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)\n",
    "    plt.title(\"Best Model Decision Boundary\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_constrained_svm_analysis(data, target_col, missing_percentage=0.2):\n",
    "    \"\"\"\n",
    "    Complete analysis pipeline combining GPLVM and cSVM\n",
    "    \"\"\"\n",
    "    # 1. Create missing data\n",
    "    data_missing = create_missing_data(data, missing_percentage)\n",
    "    \n",
    "    # 2. GPLVM imputation with uncertainty\n",
    "    imputed_data, uncertainty = gplvm_imputation(data_missing)\n",
    "    \n",
    "    # 3. Prepare data for SVM\n",
    "    X = imputed_data.drop(columns=[target_col])\n",
    "    y = imputed_data[target_col]\n",
    "    \n",
    "    # 4. Train constrained SVM\n",
    "    best_model, best_score = constrained_svm(X, y, uncertainty)\n",
    "    \n",
    "    # 5. Visualize results\n",
    "    visualize_constrained_svm(X, y, uncertainty, best_model)\n",
    "    \n",
    "    return best_model, best_score, imputed_data, uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By considering multiple possible imputed datasets, the model becomes more robust to imputation uncertainty\n",
    "\n",
    "Uncertainty Integration: The uncertainty estimates from GPLVM are directly used in the model selection process\n",
    "\n",
    "Model Selection: The best model is selected based on its performance across different possible realizations of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Development and Evaluation\n",
    "\n",
    "### Model Pipeline\n",
    "- Baseline Models:\n",
    "  - Logistic Regression\n",
    "  - Random Forest\n",
    "- Primary Model: XGBoost\n",
    "\n",
    "### Evaluation Metrics\n",
    "- Classification Metrics:\n",
    "  - AUC-ROC\n",
    "  - F1 Score\n",
    "  - Precision\n",
    "  - Recall\n",
    "- Model Stability:\n",
    "  - Feature importance consistency across missingness levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def train_evaluate_models(X_train, X_test, y_train, y_test):\n",
    "    # Define models\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "        'Random Forest': RandomForestClassifier(),\n",
    "        'XGBoost': xgb.XGBClassifier()\n",
    "    }\n",
    "    \n",
    "    # Define parameter grids for GridSearchCV\n",
    "    param_grids = {\n",
    "        'Logistic Regression': {'C': [0.1, 1, 10]},\n",
    "        'Random Forest': {'n_estimators': [100, 200], 'max_depth': [None, 10]},\n",
    "        'XGBoost': {'n_estimators': [100, 200], 'max_depth': [3, 6]}\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Perform grid search\n",
    "        grid_search = GridSearchCV(model, param_grids[name], cv=5, scoring='roc_auc')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Get best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        results[name] = {\n",
    "            'auc_roc': roc_auc_score(y_test, y_pred_proba),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred)\n",
    "        }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Interpretation\n",
    "\n",
    "### SHAP Analysis\n",
    "- Compare feature importance across different missingness levels\n",
    "- Identify stable vs. unstable features\n",
    "- Assess the impact of imputation on feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "def analyze_feature_importance(model, X, feature_names):\n",
    "    # Suppress warnings about NumPy version\n",
    "    warnings.filterwarnings('ignore', category=UserWarning)\n",
    "    \n",
    "    try:\n",
    "        # Create SHAP explainer\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        \n",
    "        # Calculate SHAP values\n",
    "        shap_values = explainer.shap_values(X)\n",
    "        \n",
    "        # Create summary plot\n",
    "        shap.summary_plot(shap_values, X, feature_names=feature_names)\n",
    "        \n",
    "        # Calculate feature importance\n",
    "        importance = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': np.abs(shap_values).mean(0)\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: SHAP analysis encountered an error: {str(e)}\")\n",
    "        print(\"Using alternative feature importance method...\")\n",
    "        \n",
    "        # Fallback to model's built-in feature importance\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importance = pd.DataFrame({\n",
    "                'feature': feature_names,\n",
    "                'importance': model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "        else:\n",
    "            # If no feature importance is available, use coefficients\n",
    "            if hasattr(model, 'coef_'):\n",
    "                importance = pd.DataFrame({\n",
    "                    'feature': feature_names,\n",
    "                    'importance': np.abs(model.coef_[0])\n",
    "                }).sort_values('importance', ascending=False)\n",
    "            else:\n",
    "                print(\"No feature importance method available for this model\")\n",
    "                return None\n",
    "    \n",
    "    return importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cpsc419)",
   "language": "python",
   "name": "cpsc419"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
