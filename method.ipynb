{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "This section outlines our approach to evaluating the impact of different imputation methods on predictive modeling for heart failure patients in MIMIC-IV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing and Feature Selection\n",
    "\n",
    "### Feature Selection Pipeline\n",
    "- Initial feature selection using LASSO regression to identify the most important predictors\n",
    "- Further refinement using XGBoost feature importance\n",
    "- Final feature set used across all imputation methods for fair comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LassoCV\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def feature_selection_pipeline(data, target_col):\n",
    "    # Split data\n",
    "    X = data.drop(columns=[target_col])\n",
    "    y = data[target_col]\n",
    "    \n",
    "    # LASSO feature selection\n",
    "    lasso = LassoCV(cv=5)\n",
    "    lasso.fit(X, y)\n",
    "    \n",
    "    # Get non-zero coefficients\n",
    "    lasso_features = X.columns[lasso.coef_ != 0]\n",
    "    \n",
    "    # XGBoost feature importance\n",
    "    xgb_model = xgb.XGBClassifier()\n",
    "    xgb_model.fit(X[lasso_features], y)\n",
    "    \n",
    "    # Get feature importance\n",
    "    importance = pd.DataFrame({\n",
    "        'feature': lasso_features,\n",
    "        'importance': xgb_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    return importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Missing Data Handling\n",
    "\n",
    "### Imputation Methods\n",
    "We evaluate three different imputation approaches:\n",
    "1. **Mean/Mode Imputation**: Simple baseline method\n",
    "2. **Regression-based Imputation**: Using predictive models for each feature\n",
    "3. **GPLVM Imputation**: Advanced deep learning-based approach\n",
    "\n",
    "### Missingness Scenarios\n",
    "- Full data (0% missing)\n",
    "- Subset with 0% missing values\n",
    "- Subset with 20% missing values\n",
    "- Subset with 40% missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def create_missing_data(data, missing_percentage):\n",
    "    # Create missing values in the dataset\n",
    "    mask = np.random.random(data.shape) < missing_percentage\n",
    "    data_missing = data.copy()\n",
    "    data_missing[mask] = np.nan\n",
    "    return data_missing\n",
    "\n",
    "def mean_mode_imputation(data):\n",
    "    # Separate numeric and categorical columns\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # Create imputers\n",
    "    numeric_imputer = SimpleImputer(strategy='mean')\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    \n",
    "    # Impute data\n",
    "    data_imputed = data.copy()\n",
    "    if len(numeric_cols) > 0:\n",
    "        data_imputed[numeric_cols] = numeric_imputer.fit_transform(data[numeric_cols])\n",
    "    if len(categorical_cols) > 0:\n",
    "        data_imputed[categorical_cols] = categorical_imputer.fit_transform(data[categorical_cols])\n",
    "    \n",
    "    return data_imputed\n",
    "\n",
    "def regression_imputation(data):\n",
    "    # Use Random Forest for imputation\n",
    "    imputer = IterativeImputer(\n",
    "        estimator=RandomForestRegressor(),\n",
    "        max_iter=10,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Impute only numeric columns\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    data_imputed = data.copy()\n",
    "    data_imputed[numeric_cols] = imputer.fit_transform(data[numeric_cols])\n",
    "    \n",
    "    return data_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Development and Evaluation\n",
    "\n",
    "### Model Pipeline\n",
    "- Baseline Models:\n",
    "  - Logistic Regression\n",
    "  - Random Forest\n",
    "- Primary Model: XGBoost\n",
    "\n",
    "### Evaluation Metrics\n",
    "- Classification Metrics:\n",
    "  - AUC-ROC\n",
    "  - F1 Score\n",
    "  - Precision\n",
    "  - Recall\n",
    "- Model Stability:\n",
    "  - Feature importance consistency across missingness levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def train_evaluate_models(X_train, X_test, y_train, y_test):\n",
    "    # Define models\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "        'Random Forest': RandomForestClassifier(),\n",
    "        'XGBoost': xgb.XGBClassifier()\n",
    "    }\n",
    "    \n",
    "    # Define parameter grids for GridSearchCV\n",
    "    param_grids = {\n",
    "        'Logistic Regression': {'C': [0.1, 1, 10]},\n",
    "        'Random Forest': {'n_estimators': [100, 200], 'max_depth': [None, 10]},\n",
    "        'XGBoost': {'n_estimators': [100, 200], 'max_depth': [3, 6]}\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Perform grid search\n",
    "        grid_search = GridSearchCV(model, param_grids[name], cv=5, scoring='roc_auc')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Get best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        results[name] = {\n",
    "            'auc_roc': roc_auc_score(y_test, y_pred_proba),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred)\n",
    "        }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Interpretation\n",
    "\n",
    "### SHAP Analysis\n",
    "- Compare feature importance across different missingness levels\n",
    "- Identify stable vs. unstable features\n",
    "- Assess the impact of imputation on feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "def analyze_feature_importance(model, X, feature_names):\n",
    "    # Suppress warnings about NumPy version\n",
    "    warnings.filterwarnings('ignore', category=UserWarning)\n",
    "    \n",
    "    try:\n",
    "        # Create SHAP explainer\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        \n",
    "        # Calculate SHAP values\n",
    "        shap_values = explainer.shap_values(X)\n",
    "        \n",
    "        # Create summary plot\n",
    "        shap.summary_plot(shap_values, X, feature_names=feature_names)\n",
    "        \n",
    "        # Calculate feature importance\n",
    "        importance = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': np.abs(shap_values).mean(0)\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: SHAP analysis encountered an error: {str(e)}\")\n",
    "        print(\"Using alternative feature importance method...\")\n",
    "        \n",
    "        # Fallback to model's built-in feature importance\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importance = pd.DataFrame({\n",
    "                'feature': feature_names,\n",
    "                'importance': model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "        else:\n",
    "            # If no feature importance is available, use coefficients\n",
    "            if hasattr(model, 'coef_'):\n",
    "                importance = pd.DataFrame({\n",
    "                    'feature': feature_names,\n",
    "                    'importance': np.abs(model.coef_[0])\n",
    "                }).sort_values('importance', ascending=False)\n",
    "            else:\n",
    "                print(\"No feature importance method available for this model\")\n",
    "                return None\n",
    "    \n",
    "    return importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cpsc419)",
   "language": "python",
   "name": "cpsc419"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
